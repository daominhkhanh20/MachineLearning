{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction Recurrent Neural Network\n",
    "Let's start with a new version of neural network for the most common system such that: Google translate,Generate text,... <br>\n",
    "In this tutorial, we'll build a small application to predict a sentence is positive or negative.We'll use model many to one following in below image\n"
   ]
  },
  {
   "source": [
    "<img src=\"image/modelBackpro.png\" width=500 height=300/>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data import train_data,test_data\n",
    "from numpy.random import rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Construct vocabulary \n",
    "We'll have to do some pre-processing data to get the data into the usable format.To start, we'll build a construct vocabulary of all worlds exist in the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "18\n['i', 'good', 'or', 'now', 'happy', 'bad', 'very', 'is', 'and', 'at', 'earlier', 'am', 'not', 'this', 'sad', 'all', 'right', 'was']\n"
     ]
    }
   ],
   "source": [
    "vocab=list(set(w for text in train_data.keys() for w in text.split(' ')))\n",
    "vocab_size=len(vocab)\n",
    "print(vocab_size)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Assign an integer index to represent for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'i': 0, 'good': 1, 'or': 2, 'now': 3, 'happy': 4, 'bad': 5, 'very': 6, 'is': 7, 'and': 8, 'at': 9, 'earlier': 10, 'am': 11, 'not': 12, 'this': 13, 'sad': 14, 'all': 15, 'right': 16, 'was': 17}\n{0: 'i', 1: 'good', 2: 'or', 3: 'now', 4: 'happy', 5: 'bad', 6: 'very', 7: 'is', 8: 'and', 9: 'at', 10: 'earlier', 11: 'am', 12: 'not', 13: 'this', 14: 'sad', 15: 'all', 16: 'right', 17: 'was'}\n"
     ]
    }
   ],
   "source": [
    "word_to_index={word:i for i,word in enumerate(vocab)}\n",
    "index_to_word={i:word for i,word in enumerate(vocab)}\n",
    "print(word_to_index)\n",
    "print(index_to_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Build one-hot coding for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_one_hot_coding(text):\n",
    "    inputs=[]\n",
    "    for word in text.split(' '):\n",
    "        v=np.zeros((vocab_size,1))\n",
    "        v[word_to_index[word]][0]=1\n",
    "        inputs.append(v)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Build RNN Model\n",
    "\n",
    "<img src=\"image/rnn.jpg\" width=500 height=200/>"
   ]
  },
  {
   "source": [
    "# 2.1 A few equations for derivation\n",
    "* $W_{xh}$ used for all $x_t$ -> $h_t$ links\n",
    "* $W_{hh}$ used for all $h_{t-1}$ -> $h_t$ links\n",
    "* $W_{hy}$ used for all $h_t$ -> $y_t$ links\n",
    "* $b_{h}$ is bias,it is added when calculating $h_t$\n",
    "* $b_{y}$ is bias,it is added when calculating $y_t$\n",
    "<br><br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* Definition for the state $h(t)$<br><br>\n",
    "    <font size=\"6\">$h_t$=tanh($W_{xh}x_t$+$W_{hh}h_{t-1}+b_h$)</font></n></n>\n",
    "\n",
    "     <font size=\"6\">$y_t$=$W_{hy}h_t+b_y$</font>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<br><br><nr>\n",
    "* <font size=\"7\">Derivation for Loss Function cross entropy</font> <br><br>\n",
    "<img src=\"image/dL_dy.png\"/>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<br><br>\n",
    "<br><br>\n",
    "* <font size=\"7\"></font> <br><br>\n",
    "<img src=\"image/dL_dWhy.png\"/>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<img src=\"image/1.png\"/>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<img src=\"image/2.png\"/>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<img src=\"image/3.png\"/>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self,input_size,output_size,hiden_size=64):\n",
    "        self.input_size=input_size\n",
    "        self.hiden_size=hiden_size\n",
    "        self.output_size=output_size\n",
    "        \n",
    "        self.Wxh=rand(self.hiden_size,self.input_size)/1000\n",
    "        self.Whh=rand(self.hiden_size,self.hiden_size)/1000\n",
    "        self.Why=rand(self.output_size,self.hiden_size)/1000\n",
    "        \n",
    "        self.bh=np.zeros((self.hiden_size,1))\n",
    "        self.by=np.zeros((self.output_size,1))\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        h=np.zeros((self.Whh.shape[0],1))      \n",
    "        self.inputs=inputs\n",
    "        self.list_h={0:h}\n",
    "\n",
    "        '''\n",
    "        if a and b are matrix, a@b <==> a.dot(b)\n",
    "        if a is matrix and b is list, a@b <==> a.dot(np.array(b).T)\n",
    "        '''\n",
    "        for i,x in enumerate(inputs):\n",
    "            h=np.tanh(self.Wxh.dot(x)+self.Whh.dot(h)+self.bh)\n",
    "            self.list_h[i+1]=h\n",
    "        y=self.Why.dot(h)+self.by\n",
    "        return y,h\n",
    "    \n",
    "    def backpropagation(self,dL_dy,learn_rate=1e-2):\n",
    "        n=len(self.inputs)\n",
    "        dL_dWhy=dL_dy.dot(self.list_h[n].T) #(2,64)\n",
    "        print(dL_dWhy.shape)\n",
    "        dL_dby=dL_dy #(2,1)\n",
    "        \n",
    "        dL_dWhh=np.zeros(self.Whh.shape)#(64,64)\n",
    "        dL_dWxh=np.zeros(self.Wxh.shape)#(64,18)\n",
    "        dL_dbh=np.zeros(self.bh.shape)#(18,1)\n",
    "        \n",
    "        dL_dh=self.Why.T.dot(dL_dy)#last state(64,1)\n",
    "        \n",
    "        for t in reversed(range(n)):\n",
    "            temp=(1-self.list_h[t+1]**2)*dL_dh\n",
    "            dL_dbh+=temp\n",
    "            dL_dWhh+=temp.dot(self.list_h[t].T)\n",
    "            dL_dWxh+=temp.dot(self.inputs[t].T)\n",
    "            dL_dh=self.Whh.dot(temp)\n",
    "            \n",
    "        #np.clip() function for prevent exploding gradient\n",
    "        for d in [dL_dWxh,dL_dWhh,dL_dbh,dL_dby]:\n",
    "            np.clip(d,-1,1,out=d)\n",
    "        \n",
    "        self.Whh-=learn_rate*dL_dWhh\n",
    "        self.Wxh-=learn_rate*dL_dWxh\n",
    "        self.bh-=learn_rate*dL_dbh\n",
    "        self.by-=learn_rate*dL_dby     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn=RNN(vocab_size,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def processingData(data):\n",
    "    items=list(data.items())\n",
    "    random.shuffle(items)\n",
    "    loss=0\n",
    "    number_correct=0\n",
    "\n",
    "    for x,y in items:\n",
    "        inputs=build_one_hot_coding(x)\n",
    "        target=int(y)\n",
    "        output,_=rnn.forward(inputs)\n",
    "        probs=softmax(output)\n",
    "        loss-=np.log(probs[target])\n",
    "        number_correct+=int(np.argmax(probs)==target)\n",
    "\n",
    "        dL_dy=probs\n",
    "        dL_dy[target]-=1\n",
    "        rnn.backpropagation(dL_dy)\n",
    "\n",
    "    return loss/len(data),number_correct/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(64, 1)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "4",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f8946df83f0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessingData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m99\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'     --- Epoch %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-75af4d350540>\u001b[0m in \u001b[0;36mprocessingData\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdL_dy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mdL_dy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdL_dy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumber_correct\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-77a3689ffcc6>\u001b[0m in \u001b[0;36mbackpropagation\u001b[0;34m(self, dL_dy, learn_rate)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackpropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdL_dy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mdL_dWhy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdL_dy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_h\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(2,18)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mdL_dby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdL_dy\u001b[0m \u001b[0;31m#(2,1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 4"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "  train_loss, train_acc = processingData(train_data)\n",
    "\n",
    "  if epoch % 100 == 99:\n",
    "    print('     --- Epoch %d' % (epoch + 1))\n",
    "    print('Train:\\tLoss %.3f | Accuracy: %.3f' % (train_loss, train_acc))\n",
    "\n",
    "    test_loss, test_acc = processingData(test_data)\n",
    "    print('Test:\\tLoss %.3f | Accuracy: %.3f' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}