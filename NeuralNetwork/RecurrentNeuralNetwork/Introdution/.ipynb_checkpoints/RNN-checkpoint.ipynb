{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction Recurrent Neural Network\n",
    "Let's start with a new version of neural network for the most common system such that: Google translate,Generate text,... <br>\n",
    "In this tutorial, we'll build a small application to predict a sentence is positive or negative.So let's start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data import train_data,test_data\n",
    "from numpy.random import rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Construct vocabulary \n",
    "We'll have to do some pre-processing data to get the data into the usable format.To start, we'll build a construct vocabulary of all worlds exist in the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "['not', 'was', 'right', 'at', 'am', 'and', 'this', 'now', 'good', 'i', 'or', 'happy', 'all', 'earlier', 'sad', 'very', 'is', 'bad']\n"
     ]
    }
   ],
   "source": [
    "vocab=list(set(w for text in train_data.keys() for w in text.split(' ')))\n",
    "vocab_size=len(vocab)\n",
    "print(vocab_size)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Assign an integer index to represent for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'not': 0, 'was': 1, 'right': 2, 'at': 3, 'am': 4, 'and': 5, 'this': 6, 'now': 7, 'good': 8, 'i': 9, 'or': 10, 'happy': 11, 'all': 12, 'earlier': 13, 'sad': 14, 'very': 15, 'is': 16, 'bad': 17}\n",
      "{0: 'not', 1: 'was', 2: 'right', 3: 'at', 4: 'am', 5: 'and', 6: 'this', 7: 'now', 8: 'good', 9: 'i', 10: 'or', 11: 'happy', 12: 'all', 13: 'earlier', 14: 'sad', 15: 'very', 16: 'is', 17: 'bad'}\n"
     ]
    }
   ],
   "source": [
    "word_to_index={word:i for i,word in enumerate(vocab)}\n",
    "index_to_word={i:word for i,word in enumerate(vocab)}\n",
    "print(word_to_index)\n",
    "print(index_to_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Build one-hot coding for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_one_hot_coding(text):\n",
    "    inputs=[]\n",
    "    for word in text.split(' '):\n",
    "        v=[]*vocab_size\n",
    "        v[word_to_index[word]]=1\n",
    "        inputs.append(v)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Build RNN Model\n",
    "\n",
    "<img src=\"rnn.jpg\" width=500 height=500/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self,input_size,hiden_size,output_size):\n",
    "        self.input_size=input_size\n",
    "        self.hiden_size=hiden_size\n",
    "        self.output_size=output_size\n",
    "        \n",
    "        self.Wxh=rand(self.hiden_size,self.input_size)/1000\n",
    "        self.Whh=rand(self.hiden_size,self.hiden_size)/1000\n",
    "        self.Why=rand(self.output_size,self.hiden_size)/1000\n",
    "        \n",
    "        self.bias_h=rand((self.hiden_size,1))/1000\n",
    "        self.bias_y=rand((self.output_size,1))/1000\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        h=np.zeros((self.hiden_size,1))\n",
    "        \n",
    "        self.last_inputs=inputs\n",
    "        self.list_h={0:h}\n",
    "        for i,x in enumerate(inputs):\n",
    "            # Note: self.Wxh@x <==> self.Whx.dot(x.T)\n",
    "            h=np.tanh(self.Wxh@x+self.Whh@h+self.bias_h)\n",
    "            self.list_h[i+1]=h\n",
    "            \n",
    "        y=self.Why@h+self.bias_y\n",
    "        \n",
    "        return y,h\n",
    "    \n",
    "    def backpropagation(self,dL_dy,learn_rate=1e-2):\n",
    "        n=len(self.list_h)\n",
    "        '''\n",
    "        dL_dy: shape(2,1)\n",
    "        dL_dWhy: shape(2,hiden_size)\n",
    "        dL_dWhh: shape (2,hiden_size)\n",
    "        '''\n",
    "        dL_Why=dl_dy.@self.list_h[n].T\n",
    "        dL_d_bias_y=dL_dy\n",
    "        \n",
    "        dL_dWhh=np.zeros(self.Whh.shape)\n",
    "        dL_dWxh=np.zeros(self.Wxh.shape)\n",
    "        dL_dbh=np.zeros(self.bias_h.shape)\n",
    "        \n",
    "        dL_dh=self.Why.T@dL_dy#last state\n",
    "        \n",
    "        for t in reversed(range(n)):\n",
    "            temp=((1-self.list_h[i+1]**2)*dL_dh)\n",
    "            dL_dh+=temp\n",
    "            dL_dWhh+=temp@self.list_h[t].T\n",
    "            dL_dWx+=temp@self.last_inputs[t].T\n",
    "            dL_dh=self.Whh@temp\n",
    "            \n",
    "        #np.clip() function for prevent exploding gradient\n",
    "        for d in [dL_dWx,dL_dWhh,dL_dbh,dL_by]:\n",
    "            np.clip(d,-1,1,out=d)\n",
    "        \n",
    "        self.Whh-=learn_rate*dL_dWhh\n",
    "        self\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
