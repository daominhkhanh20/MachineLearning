{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction Recurrent Neural Network\n",
    "Let's start with a new version of neural network for the most common system such that: Google translate,Generate text,... <br>\n",
    "In this tutorial, we'll build a small application to predict a sentence is positive or negative.We'll use model many to one following in below image\n"
   ]
  },
  {
   "source": [
    "<img src=\"image/modelBackpro.png\" width=500 height=300/>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data import train_data,test_data\n",
    "from numpy.random import rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Construct vocabulary \n",
    "We'll have to do some pre-processing data to get the data into the usable format.To start, we'll build a construct vocabulary of all worlds exist in the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "18\n['very', 'at', 'happy', 'bad', 'and', 'all', 'not', 'good', 'right', 'is', 'was', 'earlier', 'this', 'or', 'i', 'now', 'sad', 'am']\n"
     ]
    }
   ],
   "source": [
    "vocab=list(set(w for text in train_data.keys() for w in text.split(' ')))\n",
    "vocab_size=len(vocab)\n",
    "print(vocab_size)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Assign an integer index to represent for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'very': 0, 'at': 1, 'happy': 2, 'bad': 3, 'and': 4, 'all': 5, 'not': 6, 'good': 7, 'right': 8, 'is': 9, 'was': 10, 'earlier': 11, 'this': 12, 'or': 13, 'i': 14, 'now': 15, 'sad': 16, 'am': 17}\n{0: 'very', 1: 'at', 2: 'happy', 3: 'bad', 4: 'and', 5: 'all', 6: 'not', 7: 'good', 8: 'right', 9: 'is', 10: 'was', 11: 'earlier', 12: 'this', 13: 'or', 14: 'i', 15: 'now', 16: 'sad', 17: 'am'}\n"
     ]
    }
   ],
   "source": [
    "word_to_index={word:i for i,word in enumerate(vocab)}\n",
    "index_to_word={i:word for i,word in enumerate(vocab)}\n",
    "print(word_to_index)\n",
    "print(index_to_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Build one-hot coding for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_one_hot_coding(text):\n",
    "    inputs=[]\n",
    "    for word in text.split(' '):\n",
    "        v=np.zeros((vocab_size,1))\n",
    "        v[word_to_index[word]][0]=1\n",
    "        inputs.append(v)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Build RNN Model\n",
    "\n",
    "<img src=\"image/rnn.jpg\" width=500 height=200/>"
   ]
  },
  {
   "source": [
    "# 2.1 A few equations for derivation\n",
    "* $W_{xh}$ used for all $x_t$ -> $h_t$ links\n",
    "* $W_{hh}$ used for all $h_{t-1}$ -> $h_t$ links\n",
    "* $W_{hy}$ used for all $h_t$ -> $y_t$ links\n",
    "* $b_{h}$ is bias,it is added when calculating $h_t$\n",
    "* $b_{y}$ is bias,it is added when calculating $y_t$\n",
    "<br><br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* Definition for the state $h(t)$<br><br>\n",
    "    <font size=\"6\">$h_t$=tanh($W_{xh}x_t$+$W_{hh}h_{t-1}+b_h$)</font></n></n>\n",
    "\n",
    "    <font size=\"6\">$y_t$=$W_{hy}h_t+b_y$</font>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<br><br><nr>\n",
    "* <font size=\"7\">Derivation for Loss Function cross entropy</font> <br><br>\n",
    "<img src=\"image/dL_dy.png\"/>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<br><br>\n",
    "<br><br>\n",
    "* <font size=\"7\"></font> <br><br>\n",
    "<img src=\"image/dL_dWhy.png\"/>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<img src=\"image/1.png\"/>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<img src=\"image/2.png\"/>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<img src=\"image/3.png\"/>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self,input_size,output_size,hiden_size=64):\n",
    "        self.input_size=input_size\n",
    "        self.hiden_size=hiden_size\n",
    "        self.output_size=output_size\n",
    "        \n",
    "        self.Wxh=rand(self.hiden_size,self.input_size)/1000\n",
    "        self.Whh=rand(self.hiden_size,self.hiden_size)/1000\n",
    "        self.Why=rand(self.output_size,self.hiden_size)/1000\n",
    "        \n",
    "        self.bh=np.zeros((self.hiden_size,1))\n",
    "        self.by=np.zeros((self.output_size,1))\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        h=np.zeros((self.Whh.shape[0],1))      \n",
    "        self.inputs=inputs\n",
    "        self.list_h={0:h}\n",
    "\n",
    "        '''\n",
    "        if a and b are matrix, a@b <==> a.dot(b)\n",
    "        if a is matrix and b is list, a@b <==> a.dot(np.array(b).T)\n",
    "        '''\n",
    "        for i,x in enumerate(inputs):\n",
    "            h=np.tanh(self.Wxh.dot(x)+self.Whh.dot(h)+self.bh)\n",
    "            self.list_h[i+1]=h\n",
    "        y=self.Why.dot(h)+self.by\n",
    "        return y,h\n",
    "    \n",
    "    def backpropagation(self,dL_dy,learn_rate=1e-2):\n",
    "        n=len(self.inputs)\n",
    "        dL_dWhy=dL_dy.dot(self.list_h[n].T) #(2,64)\n",
    "        dL_dby=dL_dy #(2,1)\n",
    "        \n",
    "        dL_dWhh=np.zeros(self.Whh.shape)#(64,64)\n",
    "        dL_dWxh=np.zeros(self.Wxh.shape)#(64,18)\n",
    "        dL_dbh=np.zeros(self.bh.shape)#(18,1)\n",
    "        \n",
    "        dL_dh=self.Why.T.dot(dL_dy)#last state(64,1)\n",
    "        temp_value=self.Why\n",
    "        # for t in reversed(range(n)):\n",
    "        #     temp=(1-self.list_h[t+1]**2)*dL_dh\n",
    "        #     dL_dbh+=temp\n",
    "        #     dL_dWhh+=temp.dot(self.list_h[t].T)\n",
    "        #     dL_dWxh+=temp.dot(self.inputs[t].T)\n",
    "        #     dL_dh=self.Whh.dot(temp)\n",
    "        for t i reversed(range(n)):\n",
    "            temp_value=\n",
    "            \n",
    "        #np.clip() function for prevent exploding gradient\n",
    "        for d in [dL_dWxh,dL_dWhh,dL_dbh,dL_dby]:\n",
    "            np.clip(d,-1,1,out=d)\n",
    "        \n",
    "        self.Whh-=learn_rate*dL_dWhh\n",
    "        self.Wxh-=learn_rate*dL_dWxh\n",
    "        self.bh-=learn_rate*dL_dbh\n",
    "        self.by-=learn_rate*dL_dby     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn=RNN(vocab_size,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def processingData(data):\n",
    "    items=list(data.items())\n",
    "    random.shuffle(items)\n",
    "    loss=0\n",
    "    number_correct=0\n",
    "\n",
    "    for x,y in items:\n",
    "        inputs=build_one_hot_coding(x)\n",
    "        target=int(y)\n",
    "        output,_=rnn.forward(inputs)\n",
    "        probs=softmax(output)\n",
    "        loss-=np.log(probs[target])\n",
    "        number_correct+=int(np.argmax(probs)==target)\n",
    "\n",
    "        dL_dy=probs\n",
    "        dL_dy[target]-=1\n",
    "        rnn.backpropagation(dL_dy)\n",
    "\n",
    "    return loss/len(data),number_correct/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     --- Epoch 100\n",
      "Train:\tLoss 0.690 | Accuracy: 0.552\n",
      "Test:\tLoss 0.701 | Accuracy: 0.500\n",
      "     --- Epoch 200\n",
      "Train:\tLoss 0.690 | Accuracy: 0.552\n",
      "Test:\tLoss 0.701 | Accuracy: 0.500\n",
      "     --- Epoch 300\n",
      "Train:\tLoss 0.690 | Accuracy: 0.552\n",
      "Test:\tLoss 0.701 | Accuracy: 0.500\n",
      "     --- Epoch 400\n",
      "Train:\tLoss 0.690 | Accuracy: 0.552\n",
      "Test:\tLoss 0.700 | Accuracy: 0.500\n",
      "     --- Epoch 500\n",
      "Train:\tLoss 0.690 | Accuracy: 0.552\n",
      "Test:\tLoss 0.701 | Accuracy: 0.500\n",
      "     --- Epoch 600\n",
      "Train:\tLoss 0.690 | Accuracy: 0.552\n",
      "Test:\tLoss 0.700 | Accuracy: 0.500\n",
      "     --- Epoch 700\n",
      "Train:\tLoss 0.690 | Accuracy: 0.552\n",
      "Test:\tLoss 0.701 | Accuracy: 0.500\n",
      "     --- Epoch 800\n",
      "Train:\tLoss 0.690 | Accuracy: 0.552\n",
      "Test:\tLoss 0.701 | Accuracy: 0.500\n",
      "     --- Epoch 900\n",
      "Train:\tLoss 0.690 | Accuracy: 0.552\n",
      "Test:\tLoss 0.700 | Accuracy: 0.500\n",
      "     --- Epoch 1000\n",
      "Train:\tLoss 0.690 | Accuracy: 0.552\n",
      "Test:\tLoss 0.701 | Accuracy: 0.500\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "  train_loss, train_acc = processingData(train_data)\n",
    "\n",
    "  if epoch % 100 == 99:\n",
    "    print('     --- Epoch %d' % (epoch + 1))\n",
    "    print('Train:\\tLoss %.3f | Accuracy: %.3f' % (train_loss, train_acc))\n",
    "\n",
    "    test_loss, test_acc = processingData(test_data)\n",
    "    print('Test:\\tLoss %.3f | Accuracy: %.3f' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}